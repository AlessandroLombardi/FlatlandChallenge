%----------------------------------------------------------------------------------------
%	PACKAGES AND DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

% !TeX spellcheck = it
\documentclass[11pt, a4paper, hidelinks]{report}

\usepackage{anyfontsize}
%\usepackage{siunitx} % Provides the \SI{}{} and \si{} command for typesetting SI units
\usepackage{graphicx} % Required for the inclusion of images
\usepackage{subcaption}
%\usepackage{natbib} % Required to change bibliography style to APA
\usepackage{amsmath} % Required for some math elements
\usepackage[export]{adjustbox}
\usepackage{eurosym} % euro simbol
\usepackage{hyperref} % hyperlink
\usepackage[utf8]{inputenc}
\usepackage{bookmark}
\usepackage{float}
\usepackage{epigraph}
\usepackage{quoting}
\usepackage{newlfont}
\usepackage{color}
\usepackage{geometry}
\usepackage{algorithm2e}
\usepackage{titletoc}

% To add numbers in algorithm lines
\LinesNumbered

\renewcommand{\labelenumi}{\alph{enumi}.} % Make numbering in the enumerate environment by letter rather than number (e.g. section 6)

\begin{document}
\begin{titlepage}

\begin{center}
{{\Large{\textsc{Alma Mater Studiorum $\cdot$ University of Bologna}}}}
\rule[0.1cm]{15.8cm}{0.25mm}
\\\vspace{3mm}
%
%
{\Large{Dipartimento di Informatica - Scienza e Ingegneria\\
Artificial Intelligence}}


\end{center}

\vspace{20mm}

\begin{center}{
%
%
	{\LARGE{\textbf{Flatland Challenge}}}}
\end{center}

\vspace{15mm}

{\begin{center}
	 \large{Project Presentation}
\end{center}}

\vspace{32mm} \par \noindent

\begin{minipage}[t]{0.47\textwidth}
%
%
{\large{ Professor \vspace{2mm}\\{\textbf{Andrea Asperti}
}\\\\\\}}
\end{minipage}
%
\hfill
%
\begin{minipage}[t]{0.47\textwidth}\raggedleft{}{
{\large{ Students
\vspace{2mm}\\
\textbf{Alessandro Lombardi\\Fiorenzo Parascandolo} }}}
\end{minipage}

\vspace{31mm}

\begin{center}
Academic Year {2019/2020}
\end{center}

\end{titlepage}

{\tableofcontents}
\thispagestyle{empty}

\newpage
\setcounter{page}{1}

\chapter*{Introduction}

\chapter{Flatland environment}\label{ch:flatland-environment}
\addcontentsline{toc}{chapter}{Flatland}

In order to conceive a solution for this challenge we need to fully understand the environment, how does it work and how do some subtle details influence behind the scenes what we can only perceive through observations and rewards.
The following results have been evaluated on the Python package flatland-rl version 2.1.10.

\section*{The classes RailAgentStatus and EnvAgent}\label{sec:the-classes-railagentstatus-and-envagent}

\textbf{RailAgentStatus} extends Python IntEnum and assumes the following values:
\begin{itemize}
	\item READY\_TO\_DEPART (0) the agent is not in the grid yet (position is None), the prediction is to stay at the starting position.
If a MOVE\_* action is performed during this state it becomes ACTIVE\@.
	\item ACTIVE (1) the agent is in the grid (position is not None) and hasn't reached the target yet, the prediction is the remaining path.
	\item DONE (2) the agent is still in the grid (position is not None) but has already reached the target, the prediction is to stay at the target forever.
	\item DONE\_REMOVED (3) the agent has reached the target and it's removed from the grid.
\end{itemize}

\textbf{Grid4TransitionsEnum} extends Python standard IntEnum and assumes the following values: NORTH (0), EAST (1), SOUTH (2), WEST (3).
\textbf{Grid4TransitionsEnum} is used to indicate absolute directions, related to the environment, like a compass.
Possible usages are storing where the agent is facing or computing legal actions, for example including as observation a one hot encoding of the directions where the agent can move.
\\
\textbf{EnvAgent} class models the agent and encapsulates in its internal state the following attributes:
\begin{itemize}
	\item initial\_position: Tuple[int, int], initial coordinate.
	\item initial\_direction: Grid4TransitionsEnum, the initial agent facing direction.
	\item direction: Grid4TransitionsEnum, the current facing direction.
	\item target: Tuple[int, int], the final coordinate.
	\item moving: bool, True if the agent is in a moving state.
	\item speed\_data: dictionary, containing information about the agent's speed.
	\item malfunction\_data: dictionary, containing information about malfunctions.
	\item status: RailAgentStatus, the current agent status.
\end{itemize}

The speed of an agent contains the keys 'position\_fraction' used as a counter of the percentage of completion of an movement from a cell to another, 'speed' the value between 0 and 1 used to increment the 'position\_fraction' and 'transition\_action\_on\_cellexit' which contains the action to perform on the next cell, if it completes the one in the current step, otherwise in following steps may change multiple times.

The malfunction of an agent contains the keys 'malfunction' which contains how many steps are necessary to fix the agent, 'malfunction\_rate', the mean rate (average number of events in an interval) of the Poisson distribution, 'next\_malfunction' the number of steps the next malfunction will occur and 'nr\_malfunctions' the number of previous malfunctions.

\section{The class RailEnv}\label{sec:the-class-railenv}

From the documentation
\begin{quotation}
	RailEnv is an environment inspired by a (simplified version of) a rail
    network, in which agents (trains) have to navigate to their target
    locations in the shortest time possible, while at the same time cooperating
    to avoid bottlenecks.
\end{quotation}

In the \textit{step} function the number of steps is updated and if the overall task is still uncompleted, for each agent the associated reward is initially put to zero, a malfunction is tried to be induced and the specific step is performed.
The info of the agent are prepared and finally the malfunctions are ``repaired''.
Agents are handled in the order in which are passed.

\subsection{Environment Actions}\label{subsec:environment-actions}
The available actions are:
\begin{itemize}
	\item DO\_NOTHING (0) Default action if None has been provided or the value is not within this list.
If agent.moving is True then the agent will MOVE\_FORWARD\@.
    \item MOVE\_LEFT (1) If agent.moving is False then becomes True.
If it's possible turn the agent left, changing its direction, otherwise if agent.moving is True tries the action MOVE\_FORWARD\@.
    \item MOVE\_FORWARD (2) If agent.moving is False then becomes True.
It updates the direction of the agent and if the new cell is a dead-end the new direction is the opposite of the current.
    \item MOVE\_RIGHT (3) If agent.moving is False then becomes True.
If it's possible turn the agent right, changing its direction, otherwise if agent.moving is True tries the action MOVE\_FORWARD\@.
    \item STOP\_MOVING (4) If agent.moving is True then becomes False.
A penalty will be added.
Stop the agent in the current occupied cell.
\end{itemize}

\begin{algorithm}[H]
	\uIf{agent is in DONE or in DONE\_REMOVED ($1^{\text{th}}$ case)}{
		no reward is computed\\
		\Return{}
	}
	\uIf{agent is in READY\_TO\_DEPART ($2^{\text{th}}$ case)}{
		if the provided action is a MOVE\_* type and the initial cell is free the agent become ACTIVE and is initialized\\
		reward is computed\\
		\Return{}
	}
	\uIf{agent is in malfunction (($3^{\text{th}}$ case))}{
		reward is computed\\
		\Return{}
	}
	\uIf{agent is at the beginning of a cell}{
		update agent.moving considering the observations above depending on the different action types.\\
		\uIf{agent.moving}{
			the wanted action validity is first checked and if it is valid (considering also the possibility to backup from an invalid MOVE\_RIGHT or MOVE\_LEFT to a valid MOVE\_FORWARD) action is stored otherwise agent.moving becomes False and penalties are added, in this process agent.moving and agent.speed\_data['transition\_action\_on\_cellexit'] are updated
		}
	}

	\eIf{agent.moving ($4^{\text{th}}$ case)}{
		Updates the percentage of completion then if it is completely arrived on the next cell, before updating the position, the direction and clears the completion percentage it checks whether the new cell is free. Until the cell remains occupied in the future executions the agent will repeat this process.\\
		reward is computed
	}{
		reward is computed
	}
 	\caption{The \textit{\_step\_agent} algorithm}\label{alg:flatland_agent_loop}
\end{algorithm}
\noindent
\\
\\
Some useful questions:
\begin{itemize}
	\item Can agents stop during the performance of an action between two cells?
Absolutely no, it's like any other action.
	\item Are requested actions during a malfunction ignored?
Yes.
	\item Are requested actions during a not completed movement saved for after execution?
No, because the condition at line 11 is not executed and the conditions in line 16 check whether the cell is free and possibly complete agent data.
Actions are not allowed to change within the cell, each agent can only chose an action to be taken when entering a cell.
This action is then executed when a step to the next cell is valid.
	\item How is possible to understand if an agent is ready to perform an action?
The entry info\_dict["action\_required"] returned by the function \textit{step} of \textbf{RailEnv} contains True for the given agent.
This doesn't mean that the action will be successfully executed due to the presence of malfunctions or blocking agents, in this case info\_dict["action\_required"] will remain True.
	\item Does an agent which has been reached DONE be removed automatically the following step?
Although in rendering its representation may remain, the agent is removed.
It is possible to change this setting modifying the attribute remove\_agents\_at\_target of the class \textbf{RailEnv}.
	\item Does an agent automatically pass from READY\_TO\_DEPART to ACTIVE at the beginning?
No. A MOVE\_* is necessary.
	\item Do collisions occur?
No, agents check if the cell is free before moving.
Deadlocks are possible when two agents are one in front the other without any chance to change path.
\end{itemize}

\subsection{Malfunctions}\label{subsec:malfunctions}
The strategy depends on the passed \textit{malfunction\_generator\_and\_process\_data}.
A strategy can be defined using the class \textbf{MalfunctionParameters} that can be initialized with the parameters to shape the stochasticity of the environment as the malfunction rate, expressed as a probability (Poisson distribution), and minimum and maximum malfunction duration.

\begin{itemize}
	\item A malfunction can occur during the resolution of another?
No.
	\item An agent could have a malfunction during the completion of an action between two cells?
Yes, malfunctions are induced before the step and so before also action completion.
\end{itemize}

\subsection{Speed}\label{subsec:speed}
The different speed profiles (speed is between 0 and 1) can be generated setting the parameter schedule\_generator.
Speed configurations can be build using \textbf{ScheduleGenerator}s.

\subsection{Rewards}\label{subsec:rewards}
The rewards are based on the following values:
\begin{itemize}
	\item invalid\_action\_penalty which is currently set to 0, penalty for requesting an invalid action
	\item \textbf{step\_penalty} which is -1 * alpha, penalty for a time step.
	\item \textbf{global\_reward} which is 1 * beta, a sort of default penalty.
	\item stop\_penalty which is currently set to 0, penalty for stopping a moving agent
	\item start\_penalty which is currently set to 0, penalty for starting a stopped agent
\end{itemize}

The full step penalty is computed as the product between step\_penalty and agent.speed\_data['speed'].
There are different rewards for different situations:

\begin{itemize}
	\item single agents that are in DONE or in DONE\_REMOVED have zero reward ($1^{\text{th}}$ \textit{\_step\_agent} case).
	\item all agents that have finished in this episode (checked at the end of the \textit{step}) or previously (checked at the beginning of the \textit{step}), have reward equal to the global\_reward (when in \textit{step} all agents have reached their target)
	\item full step penalty is assigned when an agent is READY\_TO\_DEPART and in the current turn moves or stay there ($2^{\text{th}}$ \textit{\_step\_agent} case), or when is in malfunction ($3^{\text{th}}$ \textit{\_step\_agent} case).
	\item full step penalty plus the other penalties (invalid\_action\_penalty, stop\_penalty and start\_penalty) when the agent is finishing actions or start new ones ($4^{\text{th}}$ \textit{\_step\_agent} case).
Currently the other penalties are all set to zero.
\end{itemize}

So each train starts counting rewards since the beginning, not since it becomes ACTIVE\@.
Currently it is possible to say that rewards are always full step excluding the end of the episode and the single agents that have finished which have reward equal to 0.

\chapter{Multi-Agent Reinforcement Learning (MARL)}\label{ch:multi-agent-reinforcement-learning}
\addcontentsline{toc}{chapter}{Multi-Agent Reinforcement Learning (MARL)}

This section provides an overview of some useful works and theory behind MARL that we consider useful to suggest approaches or solutions to the Flatland Challenge.

\section*{Theoretical background}\label{sec:marl-theoretical-background}

\subsection{Introduction}\label{subsec:introduction}

\begin{quoting}[font=itshape, begintext={"}, endtext={"\cite{MARL_definition}}]
Specifically, MARL addresses the sequential decision-making problem of multiple autonomous agents that operate in a common environment, each of which aims to optimize its own long-term return by interacting with the environment and other agents.
\end{quoting}

MARL algorithms can be divided into three groups~\cite{zhang2019multiagent}:

\begin{itemize}
	\item \textbf{Fully cooperative}, where agents collaborate to optimize a common long-term return.
	\item \textbf{Fully competitive}, where the return of agents usually sum up to zero.
	\item \textbf{Mix of the two}, where both cooperative and competitive agents are involved.
\end{itemize}

We consider the Flatland Challenge a fully cooperative environment since each agent apparently compete to reach faster its destination and gain rails portions but the problem must consider the common time minimization goal.
There exist two closely related theoretical frameworks for MARL~\cite{zhang2019multiagent}:

\begin{itemize}
	\item \textbf{Markov/Stochastic Game}.
All agents share the same state and differently from classical single agent's \textbf{Markov Decision Process (MDP)}, the optimal performance of each agent is controlled not only by its own policy, but also the choices of all other players of the game.
Usually agents share a common reward function but it is possible to have different functions like in the team-average reward setting.
	\item \textbf{Decentralized POMDP (Dec-POMDP)}.
As a MDP can be extended to a \textbf{Partially Observable Markov Decision Process (POMDP)} when information that can be accessed at a given state is incomplete, similarly a Markov Game can be extended to a Dec-POMDP\@.
In multi-agent scenario, an agent may not only depend on the information it has autonomously gathered, it will also be influenced by the choices of other agents, which are partially observable.
In Dec-POMDP each agent has its own local observation of the system state, that without other agents' observations, leads to the impossibility to maintain a global belief state.
To overcome this problem, agents can exploit levels of coordination among them to obtain the full observability of a state by combining the individual observations from each member~\cite{castaneda}.
Dec-POMDP approaches are usually considered more difficult to solve than others, especially when the number of agents is greater than two.
	\item \textbf{Extensive-Form Game} inspired from computational game theory, it handles imperfect information.
It is usually used in mixed or competitive environments.
\end{itemize}

In the Flatland environment a state is surely represented by the positions and orientations of each agent in the map, since the railroad is static and agents influence it only by moving and interrupting paths.
Flatland environment allows to personalize how agents perceive the world implementing custom observations.

The involvement of Deep Learning to tackle the problem of MARL defines a new specific subject called \textbf{Multi-agent Deep Reinforcement Learning (MADRL)}.
~\cite{Hernandez_Leal_2019} presents four categories of recent MADRL works:
\begin{itemize}
	\item \textbf{Analysis of emergent behaviors}, in general, they do not propose learning algorithms, their main focus is to analyze and evaluate DRL algorithms in a multi-agent environment.
	\item \textbf{Learning communication}, they study communications techniques to share information.
	\item \textbf{Learning cooperation}, they directly explore approaches based on actions and observations to build multi-agent systems.
	\item \textbf{Agents modeling agents}, they study how agents reason about others to fulfill a task.
\end{itemize}

\subsection{Challenges}\label{subsec:challenges}

MARL frameworks inevitably adds many challenging problems on the single-agent scenario.
\begin{quoting}[font=itshape, begintext={"}, endtext={"\cite{Hernandez_Leal_2019}}]
Learning in multiagent settings is fundamentally more difficult than the single-agent case due to the presence of multiagent pathologies, e.g., the moving target problem (non-stationarity), curse of dimensionality, multiagent credit assignment, global exploration, and relative overgeneralization.
\end{quoting}

Below follow some problems that may affect the Flatland Challenge.

\subsubsection{Non-Stationarity}~\newline

\begin{quoting}[font=itshape, begintext={"}, endtext={"\cite{papoudakis2019dealing}}]
In Markov games, the state transition function $T$ and the reward function of each agent $r_i$ depend on the actions of all agents.
During the training of multiple agents, the policy of each agent changes through time.
As a result, each agents’ perceived transition and reward functions change as well.
Single-agent RL procedures which commonly assume stationarity of these functions might not quickly adapt to such changes.
\end{quoting}

In a single-agent environment, an agent is concerning only the outcome of its own actions.
In a multi-agent scenario, an agent observes not only the outcomes of its own action but also the behavior of other agents.
Agents may interact with each other and learn concurrently leading to a continuous reshape of the environment and to non-stationarity~\cite{zhang2019multiagent}.
For example the classical DQN does not provide working solutions, some derivations have been proposed to deal with this problem such as \textbf{Deep Repeated Update Q-network (DRUQN)}~\cite{castaneda}, \textbf{Deep Loosely Coupled Q-network (DLCQN)}~\cite{castaneda} and \textbf{ multi-agent concurrent DQN}.
Other techniques to adapt classical experience replay to multi-agent environment have been proposed such as \textbf{Hysteretic-DQN (HDQN)} and \textbf{Lenient-DQN (LDQN)}~\cite{Nguyen_2020}.

There are different ways to tackle the non-stationary problem, as illustrated by~\cite{papoudakis2019dealing}.
\begin{itemize}
	\item \textbf{Centralized Critic Architecture} based on an actor-critic algorithm.
The critics' training is centralized and has access to the observations and actions of all agents, while the actors' training is decentralized.
An example is the \textbf{Multi-Agent Deep Deterministic Policy Gradient (MADDPG)} algorithm.
In MADDPG each agent uses a centralized critic and a decentralized actor.
Since the training of each agent depends on the observations and actions of all the other agents, each agent perceives the environment as stationary.
	\item \textbf{Decentralized Learning} Techniques using self-play.
	\item Opponent Modelling.
	\item \textbf{Meta-Learning}.
	\item Communication.
Either accessing hidden layers as \textbf{CommNet} or feeding other agents' neural networks as \textbf{Reinforced Inter-Agent Learning (RIAL)}.
\end{itemize}

A popular alternative approach to MARL is \textbf{Independent Learning}, in which each agent independently learns its own policy, treating other agents as part of the environment.
While this method avoid some scalability problems and has been successfully used in practice it introduces a non-stationary environment from the point of view of each agent.

\subsubsection{Partial observability}~\newline
As mentioned, in the single-agent scenario this type of problem is usually modelled with a POMDP\@.
\textbf{Deep Recurrent Q-Networks (DRQN)} proposed using recurrent neural networks, in particular, Long Short-Term Memory (LSTMs) cells in DQN, to introduce a memory capability.
An extension of DRQN for multi-agent environments is \textbf{Deep Distributed Recurrent Q-network (DDRQN)}~\cite{Nguyen_2020}.
Another technique to deal with partial observability is \textbf{Deep Recurrent Policy Inference Q-network (DRPIQN)} learned by adapting network’s attention to policy features and their own Q-values at various stages of the training process.
Methods to address partial observability in Markov Games usually involve communication (RIAL and \textbf{Differentiable Inter-Agent Learning (DIAL)}) or parameter sharing (\textbf{PS-DQN}, \textbf{PS-DDPG}, \textbf{PS-A3C} and \textbf{PS-TRPO}).
Alternatively to the Markov Game, the Dec-POMDP can be used to model this type of scenario in a more classical \textbf{centralized learning for decentralized execution} fashion or in a more modern \textbf{decentralized learning for decentralized execution} way.

\subsubsection{Scalability}~\newline
To handle non-stationarity, each individual agent may need to account for the joint action space, whose dimension increases exponentially with the number of agents, this is  also referred to as the combinatorial nature of MARL\@~\cite{zhang2019multiagent}.
Many methods have been proposed to tackle this problem, one of them is the extension of \textbf{Curriculum Learning} for a multi-agent scenario.

\subsubsection{Information Structures and Training Schemes}~\newline
In the single-agent case is easier to understand what information is visible to the agent.
In Markov games is sufficient to observe the current state, while on extensive-form games agents may need to recall the history of past decisions.
In addition agents struggle to fully access information like rewards and policies of other agents, increasing the non-stationarity viewed by by individual agents~\cite{Nguyen_2020}~\cite{zhang2019multiagent}.
These considerations led to the development of different training schemes such as centralized learning for decentralized execution, which originated from the works on the Dec-POMDP setting and has been widely adopted in recent MADRL works, and \textbf{fully decentralized}.
The former has become a standard as simulators are usually involved in MARL training, there are different types of communications between agents and the central controller such as centralized learning, concurrent learning and parameter sharing\cite{Nguyen_2020}.
Usually decentralized settings may allow some sort of communication to address the non-convergence issue typical of the independent learning, this strategy is also referred as \textbf{decentralized setting with networked agents}.

\subsubsection{Multi-agent credit assignment}~\newline
% TODO

\chapter{Algorithms}\label{ch:algorithms}
\addcontentsline{toc}{chapter}{Algorithms}

This section describes the implementation and the testing of some Deep Reinforcement Learning algorithms explored in this work.

\section{PS-PPO}\label{sec:ps-ppo}
\subsection{Algorithm}\label{subsec:the-algorithm}
The algorithm is inspired by the work of Gupta et al.~\cite{ps-ppo_paper} which extends three classes of single-agent Deep Reinforcement Learning (DQN, DDPG and TRPO) to cooperative multi-agent systems.
In their work they explore the task of learning cooperative tasks in partially observable environments using an implicit communication protocol based on parameters sharing.
They illustrate that centralized approaches, based on mapping the joint observation of all the agents to a joint action, suffer from the exponential growth in the observation and actions spaces with the number of agents, while concurrent and independent suffer from the non-stationarity of the multi-agents environments and the lack of communication.
Parameter sharing represents an ideal trade off to tackle the major challenges of multi-agent systems and provide a scalable framework.
The authors suggest the policy gradient algorithm \textbf{Trust Region Policy Optimization (TRPO)} while in this work we present an alternative version based on \textbf{Proximal Policy Optimization (PPO)}.
We will mot report here all the details of the two above mentioned algorithms in single-agent setting, which can be found in the references~\cite{trpo}~\cite{ppo}.
The basic principle of Policy Gradient methods relies on the concept of gradient ascent to follow policies with the steepest increase in rewards.
However, this simple setting may lead the algorithm to get overconfidence and make bad moves that ruin almost irreversibly the progress of the training.
TRPO has been proposed to solve this issue by introducing the concept of guaranteed monotonic improvement.
Theoretically, TRPO can guarantee a policy improvement as long as it optimizes the local approximation within a trusted region, creating at each update a better policy.
PPO was conceived to improve the policy-based techniques by introducing an algorithm that attains the data efficiency and reliable performance of TRPO, while using only first-order optimization, as one of the most remarkable drawback of TRPO is its complexity.
PPO relies on clipped probability ratios, which forms a pessimistic estimate of the performance of the policy, and data sampled from applying for several time steps the policy to obtain a so called trajectory.
Trajectories are finally used to optimize the policies in multiple epochs of mini-batch updates.

\subsection{Implementation}\label{subsec:the-implementation}

This part does not describe some implementation details to exclusively show our work but also to underline some important aspects from the practical point of view which have been pointed out by the literature~\cite{ppo_implementation_1}~\cite{ppo_implementation_2} to be crucial for the overall algorithm performance.
Our studies will be guided by this outcomes taking into account that our environment is discrete and not continuous.
The majority of the following details are not even mentioned in the original PPO paper but have been discovered directly implemented in the OpenAI Baselines~\cite{ppo_baselines} as mentioned in this article~\cite{ppo-32-implementations-details}.

\subsubsection{Network architecture}

PPO belongs to the family of actor-critic algorithms, it requires the design of two different neural networks: the critic, which estimates the value function used as a baseline, and the actor, which updates the policy distribution in the direction suggested by the critic.
Some actor-critic implementations make parts of the actor and critic networks shared to decrease computational costs while others prefer to avoid sharing because it makes it harder to train and tune hyperparameters.
This is because the norm of gradients flowing back from the actor gradients and the critic gradients are at completely different scales which is often hard to calibrate.
First and last layers sizes are bound to the observation and action dimensions, while hidden layers may have a different one, cited studies have discovered that in many environments a wider value MLP network is beneficial and Tanh activation functions are better than ReLU\@.

\begin{quoting}[font=itshape, begintext={"}, endtext={"\cite{ppo_implementation_2}}]
Interestingly, the initial policy appears to have a surprisingly high impact on the training performance.
The key recipe appears is to initialize the policy at the beginning of training so that the action distribution is centered around 0
regardless of the observation and has a rather small standard deviation.
This can be achieved by initializing the policy MLP with smaller weights in the last layer so that the initial action distribution is almost independent of the observation and by introducing an offset in the standard deviation of actions.
\end{quoting}
From the quote is possible to observe the importance of network weights initialization especially in the last layer of the policy network.
In their case the action distribution is centered on 0 because action space is normalized between -1 and 1.

% TODO: initialization (C56), adam epsilon (C28) and learning rate (C24) and its decaying (C31) and Adam beta/momentum (C26)
% TODO: standard deviation of actions (C59-C62)

\subsubsection{Training setup}

The training setup is similar to the single-agent one but introduces more complexity to deal with multiple agents.
In particular in the first part each agent interact with the environment gathering some experience, called also trajectory.
Each agent's trajectory includes for each time step the related observation and the action performed, rewards and information about the termination of the problem are joined from each agent.
Trajectories' lengths depend on the chosen horizon.
For each agent the learning phase is repeated in each epoch using batches of length according to the size of batches set as an hyperparameter.
Increasing batch size should increase the speed of learning while adjust the horizon may have a great impact on the final result.

\subsubsection{Normalization and clipping}

\begin{quoting}[font=itshape, begintext={"}, endtext={"\cite{ppo_implementation_2}}]
Always use observation normalization and check if value function normalization improves performance.
Gradient clipping might slightly help but is of secondary importance.
\end{quoting}

% TODO: (C67)
We normalize advantages in each mini-batch by subtracting their mean and dividing by their standard deviation for the policy loss.

% TODO: value function normalization (C66)

Obviously as it represents a core part of the PPO algorithm the surrogate objective function is clipped using the hyperparameter epsilon.
The references mention also the usage of a clipping function for the value function loss (as present in the original PPO implementation) with poor results.

% TODO: global gradient clipping (C68)

\subsubsection{Advantage estimation and loss}

The references show how the \textbf{Generalized Advantage Estimation (GAE)} outperform the other techniques such as \textbf{N-step} and \textbf{V-trace}.

The $Q$ value can be estimated in the following way:
\begin{equation}
	V_t^{(N)} = \sum_{i = t}^{t + N - 1} \gamma^{i - t}r_i + \gamma^{N}V^{\pi}(s_{t + N})\label{eq:N-step}
\end{equation}

where $V^{\pi}$ is a value function approximator and $N$ controls the bias–variance tradeoff of the estimator.
$V^{\pi}$ tends to be biased because it is computed with a function approximator but has lower variance than the actual summed rewards since they are obtained from a single trajectory.
Bigger $N$ values results in an estimator closer to empirical returns and induce less bias and more variance.
Variance of the actual rewards typically increases the more steps away from the current step $t$ are taken, while close to $t$, the benefits of using an unbiased estimate may outweigh the variance introduced.
As $N$ increases, the variance in the estimates will likely start to become problematic, and switching to a lower variance but biased estimate can be better.
The advantage is measured subtracting from $V_t^{(N)}$ the estimated value $V^{\pi}(s_t)$.

\begin{equation}
	A_t^{(N)} = V_t^{(N)} - V^{\pi}(s_t)\label{eq:N-step_advantage}
\end{equation}

GAE was proposed as an improvement over the N-steps result~\cite{gae}.

\begin{quoting}[font=itshape, begintext={"}, endtext={"\cite{graesser2019foundations}}]
It addresses the problem of having to explicitly choose the number of steps of returns, n.
The main idea behind GAE is that instead of picking one value of n, we mix multiple values of n.
That is, calculate the advantage using a weighted average of individual advantages calculated with n = 1, 2, 3, . . . , k.
The purpose of GAE is to significantly reduce the variance of the estimator while keeping the bias introduced as low as possible.
\end{quoting}

GAE is defined in the following way:

\begin{equation}
	V_t^{(GAE)} = (1 - \lambda)\sum_{N > 0} \lambda^{N - 1}A_t^{(N)}\label{eq:GAE}
\end{equation}

where $0 < \lambda < 1$ is a hyperparameter controlling the bias–variance trade-off.

% TODO: overall loss include value and entropy (value (C56) and entropy coefficients (C))

\subsection{Testing}\label{subsec:testing}

\section{Curriculum Learning}\label{sec:meta-reinforcement-learning}

\begin{quoting}[font=itshape, begintext={"}, endtext={"\cite{bengio_curiculum}}]
The basic idea is to start small, learn easier aspects of the task or easier subtasks, and then gradually increase the difficulty level.\\
\dots\\
Deep learning methods attempt to learn feature hierarchies.
Features at higher levels are formed by the composition of lower level features.
Automatically learning multiple levels of abstraction may allow a system to induce complex functions mapping the input to the output directly from data, without depending heavily on human-crafted features.
\end{quoting}

Curriculum learning represents an effective bio-inspired strategy to improve learning.
Training with a curriculum accelerate the speed of convergence and may improve the final model performance.
Designing an efficient and effective curriculum is not always easy, incorrect choices may also affect negatively the algorithm.
To overcome this issue many techniques have been proposed.
Curriculum Learning applied to Reinforcement Learning must consider three different practical aspects: how to generate tasks, how to order tasks based on difficulty and how to perform \textbf{Transfer Learning} from task to task~\cite{narvekar2020curriculum}.
There are multiple paradigms and approaches to implement the aforementioned aspects, tasks can be structured into graphs or sequences, can be automatically or manually generated, based on the agent behaviour or not (adaptivity), while the task to learn can be one or more.
Transfer learning has been studied to speed up the learning by providing some initial knowledge rather than starting always from zero, it can be based on policies, models, value functions and more more.

\subsection{Implementation}\label{subsec:implementation}

A final important question when designing curricula is determining the stopping criteria.
Typically training is stopped when performance on the task or set of samples has converged, but another option is to train on each task for a level number of episodes or epochs.
Since more than one level can be associated with the same samples/tasks, this experience can be revisited later on in the curriculum.

\subsection{Testing}\label{subsec:testing2}

\newpage
~\nocite{*}
\bibliography{bibliography}
\bibliographystyle{plain}

\end{document}