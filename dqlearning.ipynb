{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Flatland\n",
    "!git clone https://gitlab.aicrowd.com/flatland/flatland.git/ --branch 223_UpdateEditor_55_notebooks\n",
    "%cd flatland\n",
    "!pip install -e .\n",
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Reshape\n",
    "from tensorflow.python.keras import Input\n",
    "\n",
    "\"\"\"\n",
    "An agent for Deep Q-Learning models.\n",
    "\n",
    "It encapsulates a neural network to predict Q-values and an experience-replay buffer to train the model on data batches.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SingleDQNAgent:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        env: OpenAI gym associated environment\n",
    "        optimizer: Neural Network optimizer\n",
    "        gamma: Discount\n",
    "        epsilon: Exploration factor\n",
    "    \"\"\"\n",
    "\n",
    "    REPLAY_BUFFER_MAX_LEN = 2000\n",
    "\n",
    "    def __init__(self, env, optimizer, gamma=0.6, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self._state_size = env.observation_space.n\n",
    "        self._action_size = env.action_space.n\n",
    "        self._optimizer = optimizer\n",
    "\n",
    "        self.replay_buffer = deque(maxlen=self.REPLAY_BUFFER_MAX_LEN)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Build q anf target networks\n",
    "        self.q_network = self._build_compile_model()\n",
    "        self.target_network = self._build_compile_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    \"\"\"\n",
    "    Store the experience in the Replay Buffer\n",
    "    \"\"\"\n",
    "\n",
    "    def store(self, state, action, reward, next_state, terminated):\n",
    "        self.replay_buffer.append((state, action, reward, next_state, terminated))\n",
    "\n",
    "    \"\"\"\n",
    "    Build the neural network to estimate q values\n",
    "    \"\"\"\n",
    "\n",
    "    def _build_compile_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(11,)))\n",
    "        model.add(Reshape((11,)))\n",
    "        model.add(Dense(22, activation='relu'))\n",
    "        model.add(Dense(22, activation='relu'))\n",
    "        model.add(Dense(self._action_size, activation='linear'))\n",
    "\n",
    "        model.compile(loss='mse', optimizer=self._optimizer)\n",
    "        return model\n",
    "\n",
    "    \"\"\"\n",
    "    Update the target network using the weights of the q one\n",
    "    \"\"\"\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "\n",
    "    \"\"\"\n",
    "    Decide the action to take based on the model or exploring new one\n",
    "    \n",
    "    Args:\n",
    "        state: the current state extracted from the observation\n",
    "    \"\"\"\n",
    "\n",
    "    def act(self, state):\n",
    "        # print(self.q_network.get_weights())\n",
    "        # Exploration\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "\n",
    "        # Exploitation\n",
    "        q_values = self.q_network.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    \"\"\"\n",
    "    The method used to train the model using a batch of experiences.\n",
    "\n",
    "    Args:\n",
    "        batch_size: the number of samples extracted from the Replay Buffer and used to train the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def retrain(self, batch_size):\n",
    "        if batch_size > len(self.replay_buffer):\n",
    "            raise ValueError(\"Replay Buffer length exceeded.\")\n",
    "\n",
    "        minibatch = random.sample(self.replay_buffer, batch_size)\n",
    "\n",
    "        for state, action, reward, new_state, done in minibatch:\n",
    "            target = self.q_network.predict(state)\n",
    "\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                t = self.target_network.predict(new_state)\n",
    "                target[0][action] = reward + self.gamma * np.amax(t)\n",
    "\n",
    "            self.q_network.fit(state, target, epochs=1, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.core.display import display\n",
    "from flatland.utils.rendertools import RenderTool, AgentRenderVariant\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete\n",
    "\n",
    "\"\"\"\n",
    "An OpenAI gym environment that wraps a Flatland one\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SingleAgentEnvironment(Env):\n",
    "    flatland_env = None\n",
    "    renderer = None\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        flatland_env: The Flatland environment\n",
    "        renderer: The renderer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, flatland_env, renderer=None):\n",
    "        self.flatland_env = flatland_env\n",
    "        self.renderer = renderer\n",
    "\n",
    "        self.reward_range = (-1, 1)\n",
    "        self.action_space = Discrete(5)\n",
    "        self.observation_space = Discrete(11)\n",
    "\n",
    "    \"\"\"\n",
    "    Execute an action.\n",
    "    Args:\n",
    "        action_dict: the dictionary agent -> action to perform\n",
    "    Return:\n",
    "        new_observation: The new observation for each agent\n",
    "        reward: The reward for each agent\n",
    "        done: True if an agent has concluded\n",
    "        info: Some info for each agent\n",
    "    \"\"\"\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        return self.flatland_env.step(action_dict)\n",
    "\n",
    "    \"\"\"\n",
    "    Reset the environment and return an observation\n",
    "    Returns:\n",
    "        observation: The new observation\n",
    "    \"\"\"\n",
    "\n",
    "    def reset(self):\n",
    "        return self.flatland_env.reset(regenerate_rail=False,\n",
    "                                       regenerate_schedule=False,\n",
    "                                       random_seed=True)\n",
    "\n",
    "    \"\"\"\n",
    "        Render the environment\n",
    "    \"\"\"\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # TODO: Merge both strategies (Jupyter vs .py)\n",
    "        # In .py files\n",
    "        # self.renderer.render_env(show=False, show_observations=False, show_predictions=False)\n",
    "        # In Jupyter Notebooks\n",
    "        env_renderer = RenderTool(self.flatland_env, gl=\"PILSVG\")\n",
    "        env_renderer.render_env()\n",
    "\n",
    "        image = env_renderer.get_image()\n",
    "        pil_image = Image.fromarray(image)\n",
    "        display(pil_image)\n",
    "        return image\n",
    "\n",
    "    \"\"\"\n",
    "        Reset the renderer the environment\n",
    "    \"\"\"\n",
    "    def reset_renderer(self):\n",
    "        self.renderer = RenderTool(\n",
    "            self.flatland_env,\n",
    "            gl=\"PILSVG\",\n",
    "            agent_render_variant=AgentRenderVariant.AGENT_SHOWS_OPTIONS_AND_BOX,\n",
    "            show_debug=True,\n",
    "            screen_height=700,\n",
    "            screen_width=1300)\n",
    "\n",
    "    def close_window(self):\n",
    "        self.renderer.close_window()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from flatland.core.env_observation_builder import ObservationBuilder\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Observation for a DQN based single agent.\n",
    "\n",
    "An observation is a dictionary of the form:\n",
    "{\"observations\": List[List[int],List[int],List[int]], \"position\": tuple}\n",
    "\n",
    "Similar to the Q-learning version but returns always the same structure, even impossible transitions in the form \n",
    "[0,0,0].\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SingleDQNAgentObs(ObservationBuilder):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def reset(self):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        handle: the agent index\n",
    "    \"\"\"\n",
    "    def get(self, handle=0):\n",
    "        agent = self.env.agents[handle]\n",
    "        observations = []\n",
    "\n",
    "        if agent.position:\n",
    "            possible_transitions = self.env.rail.get_transitions(*agent.position, agent.direction)\n",
    "            position = agent.position\n",
    "        else:\n",
    "            possible_transitions = self.env.rail.get_transitions(*agent.initial_position, agent.direction)\n",
    "            position = agent.initial_position\n",
    "\n",
    "        num_transitions = np.count_nonzero(possible_transitions)\n",
    "\n",
    "        if num_transitions == 1:\n",
    "            observations = [[0, 0, 0], [0, 1, 0], [0, 0, 0]]\n",
    "        else:\n",
    "            i = 0\n",
    "            for direction in [(agent.direction + i) % 4 for i in range(-1, 2)]:\n",
    "                observation = [0, 0, 0]\n",
    "                if possible_transitions[direction]:\n",
    "                    observation[i] = 1\n",
    "                observations.append(observation)\n",
    "                i = i + 1\n",
    "\n",
    "        return {\"observations\": observations, \"state\": position}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a929cd2a6fad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# Termination causes the end of the episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-fd78e579bdf8>\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# self.renderer.render_env(show=False, show_observations=False, show_predictions=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# In Jupyter Notebooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0menv_renderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRenderTool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatland_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"PILSVG\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0menv_renderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/flatland/lib/python3.8/site-packages/flatland/utils/rendertools.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, gl, jupyter, agent_render_variant, show_debug, clear_debug_text, screen_width, screen_height)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPILGL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjupyter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mgl\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"PILSVG\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPILSVG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjupyter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"] not found, switch to PILSVG\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/flatland/lib/python3.8/site-packages/flatland/utils/graphics_pil.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, jupyter, screen_width, screen_height)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_buildings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_scenery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_rail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/flatland/lib/python3.8/site-packages/flatland/utils/graphics_pil.py\u001b[0m in \u001b[0;36mload_buildings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdBuildings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msFile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdBuildingFiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpil_from_svg_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'svg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_composite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgBg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdBuildings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/flatland/lib/python3.8/site-packages/flatland/utils/graphics_pil.py\u001b[0m in \u001b[0;36mpil_from_svg_file\u001b[0;34m(self, package, resource)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpil_from_svg_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mbytestring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresource_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0mbytesPNG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvg2png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytestring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbytestring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnPixCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnPixCell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytesPNG\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfIn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mpil_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfIn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/flatland/lib/python3.8/site-packages/cairosvg/__init__.py\u001b[0m in \u001b[0;36msvg2png\u001b[0;34m(bytestring, file_obj, url, dpi, parent_width, parent_height, scale, unsafe, write_to, output_width, output_height)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mparent_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             write_to=None, output_width=None, output_height=None):\n\u001b[0;32m---> 66\u001b[0;31m     return surface.PNGSurface.convert(\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mbytestring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbytestring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_obj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mparent_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/flatland/lib/python3.8/site-packages/cairosvg/surface.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(cls, bytestring, file_obj, url, dpi, parent_width, parent_height, scale, unsafe, write_to, output_width, output_height, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m         tree = Tree(\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0mbytestring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbytestring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_obj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munsafe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             **kwargs)\n",
      "\u001b[0;32m~/.conda/envs/flatland/lib/python3.8/site-packages/cairosvg/parser.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxml_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcssselect2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mElementWrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_xml_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0mstyle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_stylesheets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melement_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_subtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/flatland/lib/python3.8/site-packages/cairosvg/css.py\u001b[0m in \u001b[0;36mparse_stylesheets\u001b[0;34m(tree, url)\u001b[0m\n\u001b[1;32m     95\u001b[0m             normal_declarations, important_declarations = parse_declarations(\n\u001b[1;32m     96\u001b[0m                 rule.content)\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mselector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcssselect2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_selector_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprelude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                 if (selector.pseudo_element is None and\n\u001b[1;32m     99\u001b[0m                         not selector.never_matches):\n",
      "\u001b[0;32m~/.conda/envs/flatland/lib/python3.8/site-packages/cssselect2/compiler.py\u001b[0m in \u001b[0;36mcompile_selector_list\u001b[0;34m(input, namespaces)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \"\"\"\n\u001b[0;32m---> 35\u001b[0;31m     return [\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mCompiledSelector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mselector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespaces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/flatland/lib/python3.8/site-packages/cssselect2/compiler.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \"\"\"\n\u001b[1;32m     35\u001b[0m     return [\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mCompiledSelector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mselector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespaces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     ]\n",
      "\u001b[0;32m~/.conda/envs/flatland/lib/python3.8/site-packages/cssselect2/compiler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parsed_selector)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCompiledSelector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparsed_selector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compile_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsed_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnever_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         self.test = eval(\n",
      "\u001b[0;32m~/.conda/envs/flatland/lib/python3.8/site-packages/cssselect2/compiler.py\u001b[0m in \u001b[0;36m_compile_node\u001b[0;34m(selector)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1'\u001b[0m  \u001b[0;31m# all([]) == True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNegationSelector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from flatland.envs.rail_env import RailEnv\n",
    "from flatland.envs.rail_generators import sparse_rail_generator\n",
    "from flatland.utils.rendertools import RenderTool, AgentRenderVariant\n",
    "from tensorflow.python.keras.optimizer_v2.adam import Adam\n",
    "\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "\"\"\"\n",
    "    Execution of the Deep Q-Learning algorithm for a single agent navigation\n",
    "\"\"\"\n",
    "\n",
    "# Render the environment\n",
    "render = True\n",
    "renderer = None\n",
    "# Print stats within the episode\n",
    "print_stats = False\n",
    "# Print stats at the end of each episode\n",
    "print_episode_stats = True\n",
    "# Frequency of episodes to print\n",
    "print_episode_stats_freq = 1\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "action_to_direction = {0: 'no-op', 1: 'left', 2: 'forward', 3: 'right', 4: 'halt'}\n",
    "\n",
    "EPISODES = 100\n",
    "TIMESTEPS = 2000\n",
    "\n",
    "WIDTH = 40\n",
    "HEIGHT = 40\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "env = RailEnv(\n",
    "    width=WIDTH,\n",
    "    height=HEIGHT,\n",
    "    rail_generator=sparse_rail_generator(\n",
    "        # Number of cities (= train stations)\n",
    "        max_num_cities=3,\n",
    "        # Distribute the cities evenly in a grid\n",
    "        grid_mode=False,\n",
    "        # Max number of rails connecting to a city\n",
    "        max_rails_between_cities=1,\n",
    "        # Number of parallel tracks in cities\n",
    "        max_rails_in_city=1,\n",
    "        seed=random_seed),\n",
    "    obs_builder_object=SingleDQNAgentObs(),\n",
    "    number_of_agents=1,\n",
    "    random_seed=random_seed)\n",
    "\n",
    "environment = SingleAgentEnvironment(env)\n",
    "agents = [SingleDQNAgent(environment, Adam(lr=0.01)) for i in range(env.number_of_agents)]\n",
    "\n",
    "agents[0].q_network.summary()\n",
    "\n",
    "\"\"\"\n",
    "Transform observation dictionary to neural network input (numpy column)\n",
    "Args:\n",
    "    observation: the observation to change\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def reshape_observation(observations):\n",
    "    for a in range(env.number_of_agents):\n",
    "        pos = [observations[a][\"state\"][0], observations[a][\"state\"][1]]\n",
    "        observation = [i for row in observations[a][\"observations\"] for i in row]\n",
    "        observation.extend(pos)\n",
    "        observations[a] = np.array(observation).reshape((-1, 11))\n",
    "\n",
    "    return observations\n",
    "\n",
    "\n",
    "# Dictionary agent -> action used in step\n",
    "action_dict = dict()\n",
    "\n",
    "# Stats\n",
    "stats = []\n",
    "\n",
    "for e in range(0, EPISODES):\n",
    "    # Reset the renderer\n",
    "    if render:\n",
    "        old_observation = environment.reset()\n",
    "\n",
    "    # Reset the environment\n",
    "    old_observations, info = environment.reset()\n",
    "    old_observations = reshape_observation(old_observations)\n",
    "\n",
    "    # Initialize variables\n",
    "    episode_reward = 0\n",
    "    terminated = False\n",
    "\n",
    "    # Episode stats\n",
    "    action_counter = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}\n",
    "\n",
    "    for time_step in range(TIMESTEPS):\n",
    "\n",
    "        # Initially False, remains False if no agent updates it\n",
    "        update_values = False\n",
    "\n",
    "        # Choose actions\n",
    "        for a in range(env.number_of_agents):\n",
    "            if info[\"action_required\"][a]:\n",
    "                update_values = True\n",
    "                action = agents[a].act(old_observations[a])\n",
    "                action_counter[action] += 1\n",
    "                if print_stats:\n",
    "                    print(\"Agent \" + str(a) + \" performs action: \" + str(action))\n",
    "            else:\n",
    "                action = 0\n",
    "            action_dict.update({a: action})\n",
    "\n",
    "        # Apply the chosen actions\n",
    "        new_observations, reward, terminated, info = environment.step(action_dict)\n",
    "\n",
    "        if print_stats:\n",
    "            print(\"Step (obs, reward, terminated, info): \")\n",
    "            print(new_observations)\n",
    "            print(reward)\n",
    "            print(terminated)\n",
    "            print(info)\n",
    "            print(\"_______________________________\")\n",
    "\n",
    "        for a in range(env.number_of_agents):\n",
    "            # Episode reward is the mean\n",
    "            episode_reward += reward[a] / env.number_of_agents\n",
    "\n",
    "            if update_values or terminated[a]:\n",
    "                # Reshape the observations to feed the network\n",
    "                new_observations = reshape_observation(new_observations)\n",
    "\n",
    "                # Store S A R S' for each agent\n",
    "                agents[a].store(old_observations[a], action_dict[a], reward[a], new_observations[a], terminated[a])\n",
    "\n",
    "                old_observations = new_observations\n",
    "\n",
    "        if render:\n",
    "            clear_output(wait=True)\n",
    "            environment.render()\n",
    "\n",
    "        # Termination causes the end of the episode\n",
    "        if terminated[\"__all__\"]:\n",
    "            for a in range(env.number_of_agents):\n",
    "                agents[a].update_target_model()\n",
    "            break\n",
    "\n",
    "        # Retrain when the batch is ready\n",
    "        for a in range(env.number_of_agents):\n",
    "            if len(agents[a].replay_buffer) > BATCH_SIZE:\n",
    "                agents[a].retrain(BATCH_SIZE)\n",
    "\n",
    "    if (e + 1) % print_episode_stats_freq == 0:\n",
    "        if print_episode_stats:\n",
    "            print(\"**********************************\")\n",
    "            print(\"Episode: {}\".format(e + 1))\n",
    "            print(\"Action counter: \" + str(action_counter))\n",
    "            print(\"Final reward: \" + str(episode_reward))\n",
    "            print(\"**********************************\")\n",
    "        stats.append({\"action_counter\": action_counter, \"episode_reward\": episode_reward})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
